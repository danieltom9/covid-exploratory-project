{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7742c77f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"covid_part2.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b82b09",
   "metadata": {},
   "source": [
    "# Final Project: COVID-19 Dataset\n",
    "## Exploring COVID-19 Data through Modeling\n",
    "## Due Date: Thursday, December 13th, 11:59 PM\n",
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with other groups about\n",
    "the project, we ask that you **write your solutions individually**. If you do\n",
    "discuss the assignments with others outside of your group please **include their names** at the top\n",
    "of your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb908b16",
   "metadata": {},
   "source": [
    "\n",
    "## This Assignment\n",
    "\n",
    "In this final project, we will investigate COVID-19 data over the past year. This data contains information about COVID-19 case counts, mortalities, vaccination rates, and various other metadata that can assist in modeling various aspects of COVID-19.\n",
    "\n",
    "Through this final project, you will demonstrate your experience with:\n",
    "* Data cleaning and EDA using Pandas\n",
    "* Unsupervised and supervised learning techniques\n",
    "* Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2811cb",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Model and analyze the temporal evolution of COVID-19 mortalities or cases using one unsupervised and one supervised technique of your choice. Interpret your models' results through visualizations, and draw insightful conclusions about the modeling of COVID-19 data.\n",
    "\n",
    "Recall that we studied linear and logistic regression, decision trees, random forests as part of supervised learning (with labels) and clustering, PCA as part of unsupervised learning (without labels). You are free to use any methods that you find suitable to answer the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c01af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import numpy as np\n",
    "from geopy import *\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import pearsonr\n",
    "import re\n",
    "\n",
    "cases = pd.read_csv('data/time_series_covid19_confirmed_US.csv') # https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\n",
    "vaccinations = pd.read_csv('data/people_vaccinated_us_timeline.csv') # https://raw.githubusercontent.com/govex/COVID-19/master/data_tables/vaccine_data/us_data/time_series/people_vaccinated_us_timeline.csv\n",
    "counties = pd.read_csv('data/co-est2020.csv', encoding='latin-1') # https://www2.census.gov/programs-surveys/popest/datasets/2010-2020/counties/totals/co-est2020.csv\n",
    "mask_use = pd.read_csv('data/mask-use-by-county.csv') # https://github.com/nytimes/covid-19-data/blob/master/mask-use/mask-use-by-county.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b18d3",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "## Data Cleaning (Again!)\n",
    "\n",
    "For this section, please copy over the appropriate answers from your previous notebook submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6efb9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Part 1: Question 1a\n",
    "\n",
    "Impute the null values in *all* the datasets with zero values or empty strings where appropriate.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccinations = vaccinations.fillna(0)\n",
    "cases = cases.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e23fb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fdc73c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Part 1: Question 1d\n",
    "\n",
    "Generate a valid FIPS code for the `counties` table.\n",
    "\n",
    "*Hint*: Refer to [this](https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt) guide on FIPS codes.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1d\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8869affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_code = counties['STATE']\n",
    "sc_str = state_code.map('{:02}'.format).astype(str)\n",
    "county_code = counties['COUNTY']\n",
    "cc_str = county_code.map('{:03}'.format).astype(str)\n",
    "FIPS = sc_str + cc_str\n",
    "counties['FIPS'] = FIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f936214a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851d0b6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Part 1: Question 1e\n",
    "\n",
    "Merge the `counties`, `cases`, and `mask_use` tables on an appropriate primary key to generate county-wise data.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1e\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3730e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties['PK'] = counties['FIPS'].astype(int)\n",
    "cases['PK'] = cases['FIPS'].astype(int)\n",
    "county_data = pd.merge(pd.merge(counties, cases, on = 'PK', how = 'inner'),\n",
    "mask_use, left_on='PK', right_on='COUNTYFP', how ='inner')\n",
    "county_data = county_data.drop(columns = ['PK', 'FIPS_y'])\n",
    "county_data = county_data.rename({'FIPS_x': 'FIPS'}, axis = 'columns')\n",
    "cases = cases.drop(columns = 'PK')\n",
    "counties = counties.drop(columns= 'PK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a0793",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb5b84",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "## Question 5: Guided Supervised Modeling\n",
    "\n",
    "This section will guide you through creating a supervised learning framework that will predict the number of COVID-19 cases per capita given various COVID-19 safety protocols that have been implemented. Then, we will investigate the bias, variance, and observational noise of this framework.\n",
    "\n",
    "Note that any answer responses without the appropriate work (i.e. code or math) will be subject to additional review and will not receive any credit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94355a0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 5a\n",
    "\n",
    "We will use county-wise mask usage data to predict the number of COVID-19 cases on September 12th, 2021. Create a visualization that shows the pairwise correlation between each combination of column in the mask usage data and the number of COVID-19 cases.\n",
    "\n",
    "*Hint*: You should be plotting 36 correlations.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5a\n",
    "points: 3\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table = county_data[['NEVER', 'RARELY', 'SOMETIMES', 'FREQUENTLY', 'ALWAYS', '9/12/21']]\n",
    "dataplot = sns.heatmap(new_table.corr(), cmap=\"YlGnBu\", annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8860eb5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 5b\n",
    "\n",
    "Train a linear regression model to predict the number of COVID-19 cases using county-wise mask usage data for September 12, 2021. Evaluate your model's RMSE on a held-out validation set with 33% of the county-wise data. When possible, make sure to set `random_state = 42` when splitting your data into training and test sets.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5b\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_q5b = county_data[['NEVER', 'RARELY', 'SOMETIMES', 'FREQUENTLY', 'ALWAYS']]\n",
    "y_q5b = county_data['9/12/21']\n",
    "# Make sure to set random_state = 42 and test_size = 0.33!\n",
    "X_q5b_train, X_q5b_test, y_q5b_train, y_q5b_test = train_test_split(X_q5b, y_q5b, test_size = 0.33, random_state = 42)\n",
    "reg = LinearRegression(fit_intercept = True)\n",
    "reg.fit(X_q5b_train, y_q5b_train)\n",
    "y_q5b_train_prediction = reg.predict(X_q5b_train)\n",
    "y_q5b_test_prediction = reg.predict(X_q5b_test)\n",
    "train_rmse_cases = np.sqrt(np.mean((y_q5b_train - y_q5b_train_prediction)**2))\n",
    "test_rmse_cases = np.sqrt(np.mean((y_q5b_test - y_q5b_test_prediction)** 2))\n",
    "train_rmse_cases, test_rmse_cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bcbd5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07da3a5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 5c\n",
    "\n",
    "Explain potential reasons the test set RMSE is much higher as compared to the training set RMSE.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5c\n",
    "points: 3\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd63a7d",
   "metadata": {},
   "source": [
    "The test RMSE may be much higher than the training RMSE because the training set is much\n",
    "larger than the test set. This means that the training set is less succeptible to error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbb97c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 5d\n",
    "\n",
    "Instead of predicting the number of COVID-19 cases, redo part (b) by predicting the number of cases per capita. Report the model's RMSE on the training and validation set.\n",
    "\n",
    "Comment on the relationship between the training and test RMSE by predicting the number of cases per capita instead of the total number of cases.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5d\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dfd858",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_q5d = county_data[['NEVER', 'RARELY', 'SOMETIMES', 'FREQUENTLY', 'ALWAYS']]\n",
    "y_q5d = county_data['9/12/21'] / county_data['POPESTIMATE2020']\n",
    "\n",
    "# Make sure to set random_state = 42 and test_size = 0.33!\n",
    "\n",
    "X_q5d_train, X_q5d_test, y_q5d_train, y_q5d_test = train_test_split(X_q5d, y_q5d, test_size = 0.33, random_state = 42)\n",
    "reg1 = LinearRegression(fit_intercept = True)\n",
    "reg1.fit(X_q5d_train, y_q5d_train)\n",
    "\n",
    "y_q5d_train_prediction = reg1.predict(X_q5d_train)\n",
    "y_q5d_test_prediction = reg1.predict(X_q5d_test)\n",
    "\n",
    "train_rmse_cpc = np.sqrt(np.mean((y_q5d_train - y_q5d_train_prediction)**2))\n",
    "test_rmse_cpc = np.sqrt(np.mean((y_q5d_test - y_q5d_test_prediction)** 2))\n",
    "\n",
    "train_rmse_cpc, test_rmse_cpc\n",
    "\n",
    "# Comment\n",
    "# The test and train RMSE are much clsoer in value when predicting cases per capita. Here,\n",
    "# we see that the train and test RMSE are around 0.035 and 0.038 respectively, which are very clsoe\n",
    "# the train and test RMSE for the total number of cases is 27834 and 62591 which are very far\n",
    "# in value. This means that predicting the cases per capita is a better choice since it is\n",
    "# experimentally less susceptible to error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbcb703",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2215f3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 5e\n",
    "\n",
    "Visualize the model outputs from part (d) by plotting the predictions $\\hat{y}$ versus the observations $y$. Comment on what the plot indicates about our linear model as a comment in the code cell.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5e\n",
    "points: 3\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e59258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for yhat versus y.\n",
    "plt.figure(figsize = (16,10))\n",
    "plt.scatter(\n",
    "x = y_q5d_test,\n",
    "y = y_q5d_test_prediction,\n",
    "s = 10,\n",
    "alpha = 0.75)\n",
    "plt.plot([0.1,0.16], [0.1, 0.16], color='orange')\n",
    "plt.xlabel('Observed $y$')\n",
    "plt.ylabel('Predicted $\\hat{y}$')\n",
    "plt.title('Predicted vs Observed Test Data')\n",
    "plt.plot\n",
    "# We can see from the plot that there isn't a clear correlation between the observed y and the\n",
    "# predicted y. It deviates from a diagonal straight line too much (a good model would have\n",
    "# this shape) so it suggests that this model isn't good enough. However, looking at the data\n",
    "# points, more points are located on around the imaginary straight line which means that while\n",
    "# this model isn't extremely accurate, it is also not extremely inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb5469",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 5f\n",
    "\n",
    "We will investigate the bias and variance of this improved model on the test set using the bias-variance decomposition to formalize the behaviour of our model. To generate an empirical estimate of the errors and the parameters in the bias-variance decomposition, train 1000 bootstrapped models on the training dataset from part (d).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5f\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b7619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "X_q5f = county_data[['NEVER', 'RARELY', 'SOMETIMES', 'FREQUENTLY', 'ALWAYS']]\n",
    "y_q5f = county_data['9/12/21'] / county_data['POPESTIMATE2020']\n",
    "for i in np.arange(1000):\n",
    "    X_q5f_train, X_q5f_test, y_q5f_train, y_q5f_test = train_test_split(X_q5f, y_q5f, test_size = 0.33)\n",
    "    X_q5f_train_added_y = X_q5f_train.copy(deep=False)\n",
    "    X_q5f_train_added_y['y_q5f_train'] = y_q5f_train\n",
    "    shuffled = X_q5f_train_added_y.sample(n=X_q5f_train_added_y.shape[0], replace=True)\n",
    "\n",
    "    reg_boot = LinearRegression(fit_intercept = True)\n",
    "    reg_boot.fit(shuffled[['NEVER', 'RARELY', 'SOMETIMES', 'FREQUENTLY', 'ALWAYS']],\n",
    "        shuffled['y_q5f_train'])\n",
    "    models.append(reg_boot)\n",
    "\n",
    "# This is for RMSE, but it is not needed for this question.\n",
    "y_q5f_train_prediction = reg_boot.predict(X_q5f_train)\n",
    "train_rmse = np.sqrt(np.mean((y_q5f_train - y_q5f_train_prediction)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5211469f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715760e0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 5g\n",
    "\n",
    "To investigate the variance in our test predictions, we sample a particular test point $(x_i, y_i)$ such that $i = 100$. In other words, we will use the 100th point in the test set from part (d), `(X_q5d_test.iloc[100], y_q5d_test.iloc[100])` as the testing point.\n",
    "\n",
    "Generate predictions and square errors for this test point for all 1000 models, and calculate the *proportion* of the *expected* square error that is captured by the model prediction variance. In other words, we wish to estimate the following quantity:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{Var}(f_\\theta(x_i))}{\\mathrm{E}_\\theta[(y_i - f_\\theta(x_i))^2]}\n",
    "$$\n",
    "\n",
    "*Hint*: Refer to the bias-variance decomposition from lecture.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5g\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ac261",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = y_q5d_test.iloc[100]\n",
    "predictions = []\n",
    "ses = []\n",
    "for model in models:\n",
    "    pred = model.predict(X_q5d_test)[100]\n",
    "    se = (pred-actual) ** 2\n",
    "    predictions.append(pred)\n",
    "    ses.append(se)\n",
    "    \n",
    "prop_var = np.var(predictions) / np.mean(ses)\n",
    "prop_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7bcc0c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b590da8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 5h\n",
    "\n",
    "Using the bias-variance decomposition, comment on how much the variance of the model contributes to the error on the sample point above. We will extend this scenario to analyze the noise term in the bias-variance decomposition, specifically with regards to this COVID-19 dataset. Consider the following:\n",
    "\n",
    "i) Assuming no observational noise (i.e. $\\epsilon = 0$), what is the *magnitude* of the empirical model bias on this sample point?\n",
    "\n",
    "ii) Clearly, there is a non-trivial amount of observational noise with COVID-19 case data simply due to how testing works and occurs. Please take a look at [this article](https://fivethirtyeight.com/features/coronavirus-case-counts-are-meaningless/) for more information. Given this infomation, explain the issues with the assumptions and result in 5h(i).\n",
    "\n",
    "iii) Recall that we typically assume $y = g(x) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma)$. In the theoretical setting for bias-variance, we have assumed $\\mu = 0, \\sigma > 0$. In this practical setting, analyze and determine how $\\epsilon$ could be modeled (as a normal distribution, but you may also consider how it could be modeled as another distribution). Are there any immediate issues with the assumptions we have made in the theoretical setting where $\\mu = 0, \\sigma > 0$? What conditions on $\\mu, \\sigma$ could be more appropriate and why?\n",
    "\n",
    "iv) Does the standard bias-variance decomposition presented in lecture hold given $\\epsilon$ being normally distributed according to your answer in part (iii)? If so, please explain why. If not, explain why it does not hold and if possible, how to modify the bias-variance decomposition to make it hold (i.e. perhaps there is an additional noise term $E[\\epsilon^3]$). \n",
    "\n",
    "*Hint*: Try to express $y = g(x) + \\epsilon$ by adding and subtracting a certain quantity.\n",
    "\n",
    "v) Intuitively, is it possible to minimize the true model bias to 0 given your $\\epsilon$ formulation in part (iii)? Why or why not? Justify your answer using part (iv) if possible.\n",
    "\n",
    "vi) Consider the infinite sample case where an oracle grants you as many samples as requested, and answer the same question in part (v). Is it possible to minimize the true model bias to 0 given your $\\epsilon$ formulation in part (iii)? Conclude with an analysis of what our modeling task can approximate using $X\\theta \\approx y$ in the finite and infinite sample case.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5h\n",
    "points: 24\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b1286-ef27-43e9-8661-22fc0b8d8e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.mean(ses) - np.var(predictions))**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c586ad-1315-4ef9-8bd7-ac4ca1222b84",
   "metadata": {},
   "source": [
    "_(i) The magnitude of the empirical model bias on this sample point can be found using the following formula:\n",
    "        (np.mean(ses) - np.var(predictions))**0.5\n",
    "        which in this case equals to around 0.04_\n",
    "        \n",
    "_(ii) Given the vast differences in testing procedures and strategies, our calculations and models may not be very accurate. The formula for magnitude of the empirical model bias   used in 5h(i) does not take these differences in testing into account. As a note, the formula for the magnitude is the square root of the difference between the MSE and the variance in predictions. Given the information in the article, we might conclude that either the mean squared error (MSE) or the variance in predictions may not be representative of the actual data, because we did not take into account the differences in the testing procedures, resulting in an inaccurate result._\n",
    "\n",
    "_(iii) Based on the article, $\\mu$ is more likely to be negative ($\\mu$ < 0) and it is not equal to 0 as we first assume. This is due to the likely systematic undercounting of COVID testing data, described in the article. This would cause the distribution to be shifted to the left and also cause the mean to be negative._\n",
    "\n",
    "_(iv) Based on the bias-variance decomposition presented in lecture,_\n",
    "$E((y - f_{\\theta}(x))^{2}) = \\sigma^{2} + var(f_{\\theta}(x)) + [E(f_{\\theta}(x) - g(x))]^{2}$\n",
    "\n",
    "_and that translates to: model risk = observed variance + model variance + (model bias)$^{2}$_\n",
    "\n",
    "_This standard bias-variance decomposition does not hold given the new model. Therefore we have to modify some things and it is shown here below._\n",
    "\n",
    "_We first define y being:_\n",
    "\n",
    "$y = g(x) + \\epsilon$ , where $\\epsilon - N(\\mu, \\sigma^{2})$\n",
    "\n",
    "_Modifying y with the $\\mu$ term that we now know is not zero:_\n",
    "\n",
    "$y = g(x) + \\mu + \\epsilon - \\mu$\n",
    "\n",
    "_Then, define:_\n",
    "\n",
    "$g'(x) = g(x) + \\mu$\n",
    "\n",
    "$\\epsilon' = \\epsilon - \\mu$\n",
    "\n",
    "_and therefore,_\n",
    "\n",
    "$y = g'(x) + \\epsilon'$\n",
    "\n",
    "_Applying this to the bias-variance decomposition above, we get:_\n",
    "\n",
    "$E((y - f_{\\theta}(x))^{2}) = \\sigma^{2} + var(f_{\\theta}(x)) + [E(f_{\\theta}(x) - g(x) + \\mu)]^{2}$\n",
    "\n",
    "_So, now we can see here that with the new model with the extra $\\mu$ term, the only difference is within the model bias itself. The observed variance and model variance stays the same and the model bias now adopts this extra $+ \\mu$ term. This means it is still the same bias but shifted by the value of $\\mu$._\n",
    "\n",
    "_(v) We think that it is not possible to minimize the true model bias to 0 given our $\\epsilon$ formulation in part (iii). From part (iv), our g'(x) here is defined as being $g(x) + \\mu$ and we have no way to find out the value of $\\mu$ (though we know that it is most likely negative, we will never know its exact value). With the value of $\\mu$ being unknown, there is no way for us to find out the value of g(x) and therefore we cannot minimize the true model bias to 0._\n",
    "\n",
    "_(vi) Similar to part (v), as we do not know the value of $\\mu$, there is no way for us to find out the value of g(x) and therefore we cannot minimize the true model bias to 0 even with infinite sample cases, let alone finite sample cases._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f5fdb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 5i\n",
    "\n",
    "Using the bias-variance decomposition for each test point, calculate the average variance and average test mean-square error across the entire test set from part (d). In other words, estimate the following quantities:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_i \\mathrm{Var}(f_\\theta(x_i))\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_i \\mathrm{E}_\\theta[(y_i - f_\\theta(x_i))^2]\n",
    "$$\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5i\n",
    "points: 5\n",
    "manual: False\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a207935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe with the 1037 points as the columns (so that we can add the point variance for each model on the rows)\n",
    "points_as_columns = pd.DataFrame(columns=np.arange(len(predictions)).astype(str))\n",
    "# adding the rows by the model's evaluation of each of the 1037 points\n",
    "for model in models:\n",
    "    pred = model.predict(X_q5d_test)\n",
    "    model_1000_series = pd.Series(data=pred, index = np.arange(len(pred)).astype(str))\n",
    "    points_as_columns = points_as_columns.append(model_1000_series, ignore_index = True)\n",
    "    # calculations\n",
    "    var = points_as_columns.var()\n",
    "    actuall = y_q5d_test.to_numpy()\n",
    "    mses = (points_as_columns.sub(actuall, axis = 'columns') ** 2).mean()\n",
    "avg_var, avg_mse = np.mean(var), np.mean(mses)\n",
    "avg_var, avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243a140",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6f530",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 5j\n",
    "\n",
    "Propose a solution to reducing the mean square error using the insights gained from the bias-variance decomposition above. What are the values of the quantities that have we estimated and what can be concluded about the remaining quantities? Please show all work that informs your analysis.\n",
    "\n",
    "Assume that the standard bias-variance decomposition used in lecture can be applied here.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5j\n",
    "points: 5\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbb2f5",
   "metadata": {},
   "source": [
    "_Looking at the previous questions, especially 5g, we see that the variance contributes very little to\n",
    "the model risk (shown by the small value of prop_var in 5g). Therefore, because this model has a\n",
    "tradeoff between bias and variance and the variance contributes very little, we can conclude that\n",
    "the bias contributes very largely to the model risk. Thus, we can reduce the mean squared error\n",
    "by decreasing the model bias. One way to do that is to increase the model complexity as higher\n",
    "complexity tends to decrease model bias. We can do that by adding more features to be analyzed._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f0f9b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "## Question 6: Open Supervised Modeling\n",
    "\n",
    "We wish to extend our modeling framework from Question 5 to make it more accurate; in other words, we wish to predict $f(x)$, a supervised learning output, based on past and current quantities. \n",
    "\n",
    "This section will serve as a rough guide on creating an autoregressive modeling framework for predicting a COVID-19 quantity of your choice (i.e. deaths, cases, vaccinations).\n",
    "\n",
    "Note that if you do not wish to pursue time-based modeling of COVID-19, you may skip parts (d), (e), and (f). That being said, you are strongly encouraged to incorporate time-based modeling into your open-ended modeling design since it constitutes a large component of the provided datasets.\n",
    "\n",
    "We will ***not*** grade these below questions individually (i.e. there are no points explicitly assigned to questions 6(a) to 6(f)); they are simply guiding questions and will be graded as part of the final project report. You should make sure to answer all of the questions (that are applicable to your open-ended modeling) in some form in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cbc6e8-7125-4ae6-9cf5-ba2925c2690b",
   "metadata": {},
   "source": [
    "### 1.9.1 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77f156-dc24-4414-9320-c2ac2195f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in election data\n",
    "#votes = pd.read_csv('data/2020_US_County_Level_Presidential_Results.csv', converters = {'county_fips': str}) \n",
    "# https://github.com/tonmcg/US_County_Level_Election_Results_08-20/blob/master/2020_US_County_Level_Presidential_Results.csv\n",
    "#votes.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1796a2f-c4fa-485c-b580-d240480c0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "\n",
    "#votes = pd.read_csv('data/2020_US_County_Level_Presidential_Results.csv') # Don't need FIPS to be a string anymore\n",
    "\n",
    "# Generate a merged dataframe (copied from below)\n",
    "#vaccinations_latest = vaccinations[vaccinations['Date'] == '2021-09-12'][['People_Fully_Vaccinated', 'People_Partially_Vaccinated', 'Province_State']]\n",
    "#merged = county_data.copy().drop('Province_State', 1)\n",
    "#merged = pd.merge(merged, vaccinations_latest, how='inner', left_on=\"STNAME\", right_on=\"Province_State\")\n",
    "\n",
    "# Append our voting data\n",
    "#merged = merged.astype({'FIPS': 'int64'})\n",
    "#merged = pd.merge(merged, votes[['county_fips', 'per_point_diff']], how='inner', left_on=\"FIPS\", right_on=\"county_fips\")\n",
    "\n",
    "# Create a weighted average of vote proportions\n",
    "#merged['prop_times_pop'] = merged['POPESTIMATE2020'] * merged['per_point_diff']\n",
    "#vac_votes_df = merged.groupby('STNAME').agg({'prop_times_pop': 'sum', 'POPESTIMATE2020': 'sum'})\n",
    "#vac_votes_df['weighted_avg'] = vac_votes_df['prop_times_pop'] / vac_votes_df['POPESTIMATE2020']\n",
    "#vac_votes_df = pd.merge(vac_votes_df, vaccinations_latest, how='inner', left_index=True, right_on='Province_State')\n",
    "#vac_votes_df['full_vac_prop'] = vac_votes_df['People_Fully_Vaccinated'] / vac_votes_df['POPESTIMATE2020']\n",
    "#vac_votes_df['part_vac_prop'] = vac_votes_df['People_Partially_Vaccinated'] / vac_votes_df['POPESTIMATE2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb8be3-5374-4fd1-8b45-3057e308a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(rc={'figure.figsize':(11.7,8.27)});\n",
    "#fig = sns.regplot(x=vac_votes_df['full_vac_prop'], y=vac_votes_df['weighted_avg'])\n",
    "#fig.set(\n",
    "#title = 'Proportion Difference in Votes versus Proportion of People Fully Vaccinated in US States',\n",
    "#xlabel = 'Proportion of People Fully Vaccinated',\n",
    "#ylabel = 'Proportion Difference in Votes'\n",
    "#);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7675fed5-9677-4b87-a1c5-ddadabfb8d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = sns.regplot(x=merged['ALWAYS'], y=merged['per_point_diff'], scatter_kws={'s':3})\n",
    "#fig.set(\n",
    "#title = 'Proportion Difference in Votes versus Proportion of People Who Always Wear Masks in US Counties',\n",
    "#xlabel = 'Proportion of People Who Always Wear Masks',\n",
    "#ylabel = 'Proportion Difference in Votes'\n",
    "#);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc49af93-8755-40cf-aa43-450096cdcddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged['log_cases'] = np.log(merged['9/12/21'])\n",
    "#cases_df = merged[np.isfinite(merged['log_cases'])]\n",
    "#fig = sns.regplot(x=cases_df['log_cases'], y=cases_df['per_point_diff'], scatter_kws={'s':3})\n",
    "#fig.set(\n",
    "#title = 'Proportion Difference in Votes versus Log Number of Covid Cases on 9/12/21 in US Counties',\n",
    "#xlabel = 'Log Number of COVID Cases',\n",
    "#ylabel = 'Proportion Difference in Votes'\n",
    "#);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded8ad2-e8ff-44f3-97c1-0c4e392a66ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged['cases_prop'] = merged['9/12/21'] / merged['POPESTIMATE2020']\n",
    "#fig = sns.regplot(x=merged['cases_prop'], y=merged['per_point_diff'], scatter_kws={'s':3})\n",
    "#fig.set(\n",
    "#title = 'Proportion Difference in Votes versus Proportion of COVID Cases on 9/12/21 in US Counties',\n",
    "#xlabel = 'Proportion of COVID cases',\n",
    "#ylabel = 'Proportion Difference in Votes'\n",
    "#);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab4a839-421f-403c-8c85-7b71a13e5b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = sns.regplot(x=np.log(merged['POPESTIMATE2020']), y=merged['per_point_diff'], scatter_kws={'s':3})\n",
    "#fig.set(\n",
    "#title = 'Proportion Difference in Votes versus Log County Population',\n",
    "#xlabel = 'Log County Population',\n",
    "#ylabel = 'Proportion Difference in Votes'\n",
    "#);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e3e64",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6a\n",
    "\n",
    "Train a baseline model where $f$ is the model described in Question 0a and $x$ is a quantity of *your* choice. Note that you may used *any* supervised learning approach we have studied; you are not limited to a linear model.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6a\n",
    "points: 0\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85819006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our dataframe with all values\n",
    "# In order to prevent the feature space from growing too large, we'll use only\n",
    "# the latest vaccination data rather than vaccination data over time, similar to\n",
    "# question 3 of part 1 of the project.\n",
    "#vaccinations_latest = vaccinations[vaccinations['Date'] =='2021-09-12'][['People_Fully_Vaccinated', 'People_Partially_Vaccinated','Province_State']]\n",
    "#merged = county_data.copy().drop('Province_State', 1)\n",
    "#merged = pd.merge(merged, vaccinations_latest, how='inner', left_on=\"STNAME\", right_on=\"Province_State\")\n",
    "\n",
    "# Append our voting data\n",
    "#merged = merged.astype({'FIPS': 'int64'})\n",
    "#merged = pd.merge(merged, votes[['county_fips', 'per_point_diff']], how='inner', left_on=\"FIPS\", right_on=\"county_fips\")\n",
    "\n",
    "# Convert number of vaccinations to proportion as seen in EDA\n",
    "#grouped = merged.groupby('STNAME', as_index=False).agg({'People_Fully_Vaccinated': 'first', 'People_Partially_Vaccinated':'first', 'POPESTIMATE2020': 'sum'})\n",
    "#grouped['People_Fully_Vaccinated_Prop'] = grouped['People_Fully_Vaccinated'] / grouped['POPESTIMATE2020']\n",
    "#grouped['People_Partially_Vaccinated_Prop'] = grouped['People_Partially_Vaccinated'] / grouped['POPESTIMATE2020']\n",
    "#grouped = grouped.drop(['People_Fully_Vaccinated', 'People_Partially_Vaccinated', 'POPESTIMATE2020'], 1)\n",
    "#merged = pd.merge(merged, grouped, how='inner', left_on='STNAME', right_on='STNAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1976f74-6575-4e09-b061-8e051f094e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we drop all columns that are infeasible or nonsensical for modeling. These consist of mostly identifiers\n",
    "#data = merged.drop(\n",
    "#['SUMLEV', 'REGION', 'DIVISION', 'STATE', 'COUNTY', 'STNAME', 'CTYNAME', 'FIPS', 'county_fips', 'COUNTYFP', 'Province_State',\n",
    "#'UID', 'iso2', 'iso3', 'Admin2', 'Country_Region', 'code3', 'Combined_Key'], 1)\n",
    "\n",
    "# Rename longitude\n",
    "#data = data.rename(columns={'Long_': 'Long'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0385028f-7ad0-4a46-824c-8fedaa99cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our training and testing sets\n",
    "# Our y value will be the difference in proportion between republican votes and democrat votes (ie republican_votes - democrat_votes)\n",
    "#X = data.drop('per_point_diff', 1)\n",
    "#y = data[['per_point_diff']]\n",
    "#X_train_full, X_test_full, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)\n",
    "\n",
    "# We drop the proportion for now as it's a linear combination of 2 columns. We'll use it in later models.\n",
    "#X_train = X_train_full.drop(['People_Fully_Vaccinated_Prop', 'People_Partially_Vaccinated_Prop'], 1)\n",
    "#X_test = X_test_full.drop(['People_Fully_Vaccinated_Prop', 'People_Partially_Vaccinated_Prop'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6810a-fa44-459f-853f-57e0204e1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Fit our base model with all feasible features\n",
    "#base_model = LinearRegression()\n",
    "#base_model.fit(X_train, y_train)\n",
    "#y_pred = base_model.predict(X_train)\n",
    "#rmse = mean_squared_error(y_train, y_pred) ** 0.5\n",
    "#print(\"Base Model Training RMSE: \", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b8a40",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6b\n",
    "\n",
    "Improve your model from part (a). Specify the supervised model you choose and write $f(x)$ as a function of the chosen features and parameters in your model. Justify why you chose these features and how you expect they will correlate with the output you wish to predict.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6b\n",
    "points: 0\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab91b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LassoCV # Use LassoCV to help us choose alpha\n",
    "\n",
    "# One way to select features is to use LASSO to zero out any redundant features in our feature space\n",
    "#lasso = LassoCV(cv=5, random_state=0)\n",
    "#lasso.fit(X_train, y_train)\n",
    "#lasso_coeffs = lasso.coef_\n",
    "#lasso_features = X_train.columns[lasso_coeffs > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a9a25c-c62e-41f5-84c2-28b450227782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4c7ef8-8895-4255-9206-14a1dc215a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_lasso = X_train[lasso_features]\n",
    "#model_1 = LinearRegression()\n",
    "#model_1.fit(X_train_lasso, y_train)\n",
    "#y_pred = model_1.predict(X_train_lasso)\n",
    "#rmse = mean_squared_error(y_train, y_pred) ** 0.5\n",
    "#print(\"LASSO Model Training RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85ef4d-1be8-421d-a140-406094decbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import RidgeCV # Use RidgeCV to help us choose alpha\n",
    "\n",
    "# We can also try ridge regression as another form of feature penalization\n",
    "#model_2 = RidgeCV(cv=5, alphas=np.arange(0, 3.01, 0.1))\n",
    "#model_2.fit(X_train, y_train)\n",
    "#y_pred = model_2.predict(X_train)\n",
    "#rmse = mean_squared_error(y_train, y_pred) ** 0.5\n",
    "#print(\"Ridge Model (No TS) Training RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86a9d7-67c7-4029-9a1d-ee5e8679c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series data is autocorrelated, so we can try a model that only has covid data from a single date (2021-09-12)\n",
    "# such that it matches with our vaccination data date and only use a single population estimate.\n",
    "# Correlation between features generally isn't great for linear regression which is why we're trying this approach.\n",
    "#wanted_features = [\n",
    "#'POPESTIMATE2020',\n",
    "#'Lat',\n",
    "#'Long',\n",
    "#'9/12/21',\n",
    "#'NEVER',\n",
    "#'RARELY',\n",
    "#'SOMETIMES',\n",
    "#'FREQUENTLY',\n",
    "#'ALWAYS',\n",
    "#'People_Fully_Vaccinated',\n",
    "#'People_Partially_Vaccinated'\n",
    "#]\n",
    "#X_train_no_ts = X_train[wanted_features]\n",
    "#model_3 = LinearRegression()\n",
    "#model_3.fit(X_train_no_ts, y_train)\n",
    "#y_pred = model_3.predict(X_train_no_ts)\n",
    "#rmse = mean_squared_error(y_train, y_pred) ** 0.5\n",
    "#print(\"No Time Series Model Training RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1478aa5-fe25-432f-8986-4cc10e3c5252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can once again try LASSO over this smaller feature space\n",
    "#lasso_no_ts = LassoCV(cv=5, random_state=0)\n",
    "#lasso_no_ts.fit(X_train_no_ts, y_train)\n",
    "#lasso_no_ts_coeffs = lasso_no_ts.coef_\n",
    "#lasso_no_ts_features = X_train_no_ts.columns[lasso_no_ts_coeffs > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b883d-751f-47cd-bccf-f4bbc71475e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso_no_ts_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a6417-3c52-4cca-866a-ed57f9a41496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_lasso_no_ts = X_train_no_ts[lasso_no_ts_features]\n",
    "#model_4 = LinearRegression()\n",
    "#model_4.fit(X_train_lasso_no_ts, y_train)\n",
    "#y_pred = model_4.predict(X_train_lasso_no_ts)\n",
    "#rmse = mean_squared_error(y_train, y_pred) ** 0.5\n",
    "#print(\"LASSO Model (No TS) Training RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e7541-ab54-4d3d-9391-64d28854f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import RidgeCV # Use RidgeCV to help us choose alpha\n",
    "## We can also try ridge regression as another form of feature penalization\n",
    "#model_5 = RidgeCV(cv=5, alphas=np.arange(0, 5.01, 0.1))\n",
    "#model_5.fit(X_train_no_ts, y_train)\n",
    "#y_pred = model_5.predict(X_train_no_ts)\n",
    "#rmse = mean_squared_error(y_train, y_pred) ** 0.5\n",
    "#print(\"Ridge Model (No TS) Training RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4c5d9-aca4-4c9a-b5ae-405897a820e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on EDA from part 1 as well as EDA from this part, we can manually choose variables.\n",
    "# We try and choose features that are not corrleated with each other at all.\n",
    "# Additionally, we saw that proportion worked better than raw numbers for vaccination in EDA,\n",
    "# so we change to using proportions.\n",
    "#wanted_features_eda = [\n",
    "#'9/12/21',\n",
    "#'ALWAYS',\n",
    "#'People_Fully_Vaccinated_Prop'\n",
    "#]\n",
    "#X_train_eda_1 = X_train_full[wanted_features_eda]\n",
    "#model_6 = LinearRegression()\n",
    "#model_6.fit(X_train_eda_1, y_train)\n",
    "#y_pred = model_6.predict(X_train_eda_1)\n",
    "#rmse = mean_squared_error(y_train, y_pred) ** 0.5\n",
    "#print(\"EDA Model 1 Training RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ab78c-bd6b-4e33-8564-6ea3031914df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the same thing but with partially vaccinated proportion rather than fully vaccinated proportion since\n",
    "# LASSO tends to choose partially vaccinated as an important feature.\n",
    "#wanted_features_eda = [\n",
    "#'9/12/21',\n",
    "#'ALWAYS',\n",
    "#'People_Partially_Vaccinated_Prop'\n",
    "#]\n",
    "#X_train_eda_2 = X_train_full[wanted_features_eda]\n",
    "#model_7 = LinearRegression()\n",
    "#model_7.fit(X_train_eda_2, y_train)\n",
    "#y_pred = model_7.predict(X_train_eda_2)\n",
    "#rmse = mean_squared_error(y_train, y_pred) ** 0.5\n",
    "#print(\"EDA Model 2 Training RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4614faa-dce3-438c-a4df-68debfc6eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression tended to lower RMSE previously, so we can also try ridge for the above 2 models.\n",
    "#model_8 = RidgeCV(cv=5, alphas=np.arange(0, 10.01, 0.1))\n",
    "#model_8.fit(X_train_eda_1, y_train)\n",
    "#y_pred = model_8.predict(X_train_eda_1)\n",
    "#rmse = mean_squared_error(y_train, y_pred) ** 0.5\n",
    "#print(\"EDA Ridge Model 1 Training RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89a68c-c594-41ff-a72f-29a971f90c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_9 = RidgeCV(cv=5, alphas=np.arange(0, 10.01, 0.1))\n",
    "#model_9.fit(X_train_eda_2, y_train)\n",
    "#y_pred = model_9.predict(X_train_eda_2)\n",
    "#rmse = mean_squared_error(y_train, y_pred) ** 0.5\n",
    "#print(\"EDA Ridge Model 2 Training RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8a30e-4062-4aed-8618-cef84d0bde5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import cross_val_score\n",
    "#import sklearn\n",
    "# Now we can do cross validation on our models and find the best performing one\n",
    "#models_q6 = [\n",
    "#(base_model, X_train),\n",
    "#(model_1, X_train_lasso),\n",
    "#(model_2, X_train),\n",
    "#(model_3, X_train_no_ts),\n",
    "#(model_4, X_train_lasso_no_ts),\n",
    "#(model_5, X_train_no_ts),\n",
    "#(model_6, X_train_eda_1),\n",
    "#(model_7, X_train_eda_2),\n",
    "#(model_8, X_train_eda_1),\n",
    "#(model_9, X_train_eda_2)\n",
    "#]\n",
    "#scores = []\n",
    "#count = 1\n",
    "#for pair in models_q6:\n",
    "#    model = pair[0]\n",
    "#    features = pair[1]\n",
    "#    cur_model_scores = cross_val_score(model, features, y_train, scoring='neg_root_mean_squared_error', cv=5)\n",
    "#    scores.append((cur_model_scores.mean() * -1))\n",
    "#    print(\"Finished scoring model {} out of {}\".format(count, len(models)))\n",
    "#    count += 1\n",
    "#model_score_pairs = []\n",
    "#for i in range(len(scores)):\n",
    "#    model_score_pairs.append((i, scores[i]))\n",
    "#model_score_pairs.sort(key = lambda x: x[1])\n",
    "#print(\"Model Rankings (Best to Worst)\")\n",
    "#print(\"==================================\")\n",
    "#for pair in model_score_pairs:\n",
    "#    num = pair[0]\n",
    "#    rmse = pair[1]\n",
    "#    if num == 0:\n",
    "#        name = 'base_model'\n",
    "#    else:\n",
    "#        name = 'model_{}'.format(num)\n",
    "#    print('{} RMSE: {}'.format(name, rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28de6f8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6c\n",
    "\n",
    "If applicable, write an equation or rule for the prediction function $f(x)$; if this is infeasible, make sure to visualize your model parameters in some way. Interpret your improved model's optimal parameters (*hint*: refer to 1aiii), and compare these parameters to those of the baseline model. Comment on whether the parameters follow physical intuition given the nature of the prediction task.\n",
    "\n",
    "For example, if you chose to use a decision tree, you may interpret the generated rules.\n",
    " \n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6c\n",
    "points: 0\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0defa5",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b89e46",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6d\n",
    "\n",
    "Discuss your improved model's performance on both short-term and long-term time scales using a metric of your choice (*hint:* we're using an autoregressive model). In other words, given $x_t$, we wish to predict $\\hat{x}_{t+k}$, and plot the error of these predictions for two $k$ values of your choice. You may use any reasonable interpretation of short-term and long-term predictions; an initial suggestion is to use 2-day predictions and 2-week predictions.\n",
    "\n",
    "Compare the performance of this model on both timescales with the baseline model.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6d\n",
    "points: 0\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e7549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wanted_features = [\n",
    "#'POPESTIMATE2020',\n",
    "#'Lat',\n",
    "#'Long_',\n",
    "#'9/12/21',\n",
    "#'NEVER',\n",
    "#'RARELY',\n",
    "#'SOMETIMES',\n",
    "#'FREQUENTLY',\n",
    "#'ALWAYS',\n",
    "#'People_Fully_Vaccinated',\n",
    "#'People_Partially_Vaccinated'\n",
    "#]\n",
    "#with_preds = merged.copy()\n",
    "#with_preds[wanted_features]\n",
    "#y_pred = model_3.predict(with_preds[wanted_features])\n",
    "#with_preds['predictions'] = y_pred\n",
    "#modded_fips = []\n",
    "#for fip in with_preds['county_fips']:\n",
    "#    fip = str(fip)\n",
    "#    if len(fip) < 5:\n",
    "#        fip = '0' + fip\n",
    "#    modded_fips.append(fip)\n",
    "#with_preds['county_fips'] = modded_fips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ba1ff2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6e\n",
    "\n",
    "Plot and describe the error for both the baseline and improved models as a function of time. In other words, given $x_t$, we wish to predict $\\hat{x}_{t+k}$, and plot the error of these predictions for all $k$.\n",
    "\n",
    "Consider how and why the performance of the model degrades as a function of time using the rate of growth in the error.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6e\n",
    "points: 0\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526caa49",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6f\n",
    "\n",
    "Consider a modification to the model $f(x) = x_{t+1}$ where instead $f(x) = [x_{t+1}, x_{t+2}, ..., x_{t+m}]$ for some $m > 1$. In other words, using the features $x$ that contain past and present quantities, our model *explicitly* predicts values for $m$ days in the future rather than simply the next day (i.e. $m = 1$). \n",
    "\n",
    "Train the baseline and improved model using $m = 5$ and $m = 10$. Evaluate and visualize the predictive accuracy of both models.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6f\n",
    "points: 0\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8bc5f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82aee6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecfb50d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbeabf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5501bd",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
